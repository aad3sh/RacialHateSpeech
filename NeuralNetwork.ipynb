{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8iXPQ5BY-7u"
   },
   "source": [
    "## Comparing Single and Bi-Directional Long Short-Term Memory Units\n",
    "\n",
    "In this notebook, we will be comparing two models - \n",
    "\n",
    "1. A Single LSTM unit \n",
    "2. A Bi-Directional LSTM unit\n",
    "\n",
    "We compare the output of the two models by looking at the micro-average and macro-average values of the precision, recall and F1 scores.\n",
    "\n",
    "HOW TO RUN - \n",
    "1. Select Cell from the list of Menu\n",
    "2. Run All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9THX8VE3ZY4Z"
   },
   "source": [
    "Importing all essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "Gj0FE_B2ZW6U"
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HqQSXqrHDdwA",
    "outputId": "8b0a32c2-a869-46ad-9ddb-f7a01a89b5c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7e8d193f-b556-43e6-afad-43e9046eb1d9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e8d193f-b556-43e6-afad-43e9046eb1d9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7e8d193f-b556-43e6-afad-43e9046eb1d9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7e8d193f-b556-43e6-afad-43e9046eb1d9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the dataset\n",
    "dataset = pd.read_csv('labeled_data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "WldnuR8WDdyx"
   },
   "outputs": [],
   "source": [
    "#Removing useless columns to only include the tweets and their labels\n",
    "dt_transformed = dataset[['class', 'tweet']]\n",
    "y = np.array(dt_transformed['class']) #output labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0R21X9hfaPZQ"
   },
   "source": [
    "Importing all NLTK libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnpSkkWSD_Zq",
    "outputId": "6e5d6eed-a551-4915-b67c-ef0383bb0bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "Ek_CRHlIDd3W"
   },
   "outputs": [],
   "source": [
    "#Data cleaning process. Cleaning the tweets in the dataframe and storing the cleaned text in a list.\n",
    "corpus = []\n",
    "for i in range(0, len(y)):\n",
    "  text = re.sub('[^a-zA-Z]', ' ', dt_transformed['tweet'][i])\n",
    "  text = text.lower() #coverting the text to lower case\n",
    "  text = re.sub(r'\\$\\w*', '', text) #removing special characters\n",
    "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text) #removing hyerlinks in the tweet\n",
    "  text = re.sub(r'#', '', text) #removing hashtags in the tweet\n",
    "  text = text.split()\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')\n",
    "  all_stopwords.remove('not')\n",
    "  text = [ps.stem(word) for word in text if not word in set(all_stopwords)] #removing stopwords and taking stem of the word\n",
    "  text = ' '.join(text)\n",
    "  corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "e5A8n0EhH3Xe"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "\n",
    "max_words = 5000 #setting a random maximum words count. 5000 found more optimal than 7000\n",
    "#Max tweet length is 280 characters and as observed in DataVisualization, max tweets are less than 200 characters\n",
    "max_len = 200 #setting a random\n",
    "\n",
    "#Implementing keras' tokenizer and text_to_sentences to turn the \n",
    "#string data into 3D float vector embeddings to train the neural network on.\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "#using pad_sequences to pad words for sentences less than the max_words length\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "8P7O0nCNSHW0"
   },
   "outputs": [],
   "source": [
    "#one-hot encoder to encode the three outputs - 0,1,2 into encoded vector outputs\n",
    "labels = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "EDcw5i1FKO6E"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#creating training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXpEVWoKc1VO",
    "outputId": "c75fd0e3-f104-4b34-8db8-951f5f041a3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "581/581 [==============================] - 70s 117ms/step - loss: 0.4969 - accuracy: 0.8319 - val_loss: 0.3243 - val_accuracy: 0.8962\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 52s 89ms/step - loss: 0.3026 - accuracy: 0.9027 - val_loss: 0.2971 - val_accuracy: 0.9025\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 48s 82ms/step - loss: 0.2571 - accuracy: 0.9134 - val_loss: 0.2767 - val_accuracy: 0.9007\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 47s 80ms/step - loss: 0.2225 - accuracy: 0.9248 - val_loss: 0.2822 - val_accuracy: 0.8978\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 48s 82ms/step - loss: 0.1984 - accuracy: 0.9319 - val_loss: 0.2861 - val_accuracy: 0.9027\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 46s 79ms/step - loss: 0.1798 - accuracy: 0.9368 - val_loss: 0.2994 - val_accuracy: 0.8974\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 46s 79ms/step - loss: 0.1617 - accuracy: 0.9445 - val_loss: 0.3155 - val_accuracy: 0.8930\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 45s 78ms/step - loss: 0.1527 - accuracy: 0.9467 - val_loss: 0.3252 - val_accuracy: 0.8982\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 45s 77ms/step - loss: 0.1468 - accuracy: 0.9489 - val_loss: 0.3418 - val_accuracy: 0.8965\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 45s 78ms/step - loss: 0.1358 - accuracy: 0.9530 - val_loss: 0.3565 - val_accuracy: 0.8907\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 45s 78ms/step - loss: 0.1266 - accuracy: 0.9563 - val_loss: 0.3623 - val_accuracy: 0.8919\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 46s 79ms/step - loss: 0.1252 - accuracy: 0.9557 - val_loss: 0.3942 - val_accuracy: 0.8859\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 45s 78ms/step - loss: 0.1238 - accuracy: 0.9562 - val_loss: 0.3871 - val_accuracy: 0.8882\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 45s 78ms/step - loss: 0.1151 - accuracy: 0.9593 - val_loss: 0.4214 - val_accuracy: 0.8843\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 45s 78ms/step - loss: 0.1144 - accuracy: 0.9588 - val_loss: 0.4146 - val_accuracy: 0.8878\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 45s 77ms/step - loss: 0.1154 - accuracy: 0.9591 - val_loss: 0.4154 - val_accuracy: 0.8852\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 45s 78ms/step - loss: 0.1066 - accuracy: 0.9623 - val_loss: 0.4470 - val_accuracy: 0.8851\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 45s 78ms/step - loss: 0.1067 - accuracy: 0.9617 - val_loss: 0.4308 - val_accuracy: 0.8825\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 45s 77ms/step - loss: 0.1047 - accuracy: 0.9626 - val_loss: 0.4429 - val_accuracy: 0.8830\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 45s 77ms/step - loss: 0.1002 - accuracy: 0.9634 - val_loss: 0.4454 - val_accuracy: 0.8873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc4f2b9cd0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "#creating the model \n",
    "model_simple = Sequential()\n",
    "model_simple.add(layers.Embedding(max_words, 20))\n",
    "model_simple.add(layers.LSTM(15,dropout=0.5))\n",
    "#since output layer has three classes\n",
    "model_simple.add(layers.Dense(3,activation='softmax'))\n",
    "#hyperparameters can be changed as per need. I am using adam optimizer for now as it is most common\n",
    "model_simple.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#number of epochs can be increased. I have kept it at 20 to reduce training time\n",
    "model_simple.fit(X_train, y_train, epochs=20,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nFpCH1_dVGt",
    "outputId": "24e056e4-6b36-4dc9-e451-9baa0183e862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 - 3s - loss: 0.4454 - accuracy: 0.8873 - 3s/epoch - 17ms/step\n"
     ]
    }
   ],
   "source": [
    "#using evaluate function to get loss and validation accuracy obtained\n",
    "simple_loss, simple_acc = model_simple.evaluate(X_test, y_test, verbose=2)\n",
    "y_pred = model_simple.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "aBoIg4OjdVJ1"
   },
   "outputs": [],
   "source": [
    "#converting the output predicted y, which is a vector of probabilities into a one-hot encoded output\n",
    "for i in range(len(y_pred)):\n",
    "  for j in range(3):\n",
    "    if(y_pred[i][j]<0.5):\n",
    "      y_pred[i][j]=0.0\n",
    "    else:\n",
    "      y_pred[i][j]=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlUuVDqNiPCK"
   },
   "source": [
    "Output from a simple Single LSTM model ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "VuLzmubodVMu",
    "outputId": "b3bbd790-e37c-491a-ae9b-b43abec9341c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.38      0.19      0.26       359\\n           1       0.92      0.95      0.93      4800\\n           2       0.83      0.83      0.83      1037\\n\\n   micro avg       0.89      0.89      0.89      6196\\n   macro avg       0.71      0.66      0.67      6196\\nweighted avg       0.87      0.89      0.88      6196\\n samples avg       0.89      0.89      0.89      6196\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classification_report(y_test, y_pred,zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4K_UqbQEF7GG",
    "outputId": "27a5bae4-836d-41f0-880c-52cbb1bb971e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "581/581 [==============================] - 126s 200ms/step - loss: 0.4542 - accuracy: 0.8406 - val_loss: 0.3317 - val_accuracy: 0.8906\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 81s 139ms/step - loss: 0.3279 - accuracy: 0.8922 - val_loss: 0.3123 - val_accuracy: 0.8965\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 80s 137ms/step - loss: 0.3058 - accuracy: 0.8976 - val_loss: 0.3038 - val_accuracy: 0.8911\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 111s 191ms/step - loss: 0.2839 - accuracy: 0.9035 - val_loss: 0.2972 - val_accuracy: 0.9019\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 86s 148ms/step - loss: 0.2644 - accuracy: 0.9102 - val_loss: 0.2705 - val_accuracy: 0.9030\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 83s 144ms/step - loss: 0.2569 - accuracy: 0.9112 - val_loss: 0.2688 - val_accuracy: 0.9041\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 80s 137ms/step - loss: 0.2513 - accuracy: 0.9146 - val_loss: 0.2760 - val_accuracy: 0.9062\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 76s 131ms/step - loss: 0.2466 - accuracy: 0.9171 - val_loss: 0.2673 - val_accuracy: 0.9091\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 78s 134ms/step - loss: 0.2438 - accuracy: 0.9177 - val_loss: 0.2771 - val_accuracy: 0.9066\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 79s 135ms/step - loss: 0.2397 - accuracy: 0.9186 - val_loss: 0.2659 - val_accuracy: 0.9087\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 76s 132ms/step - loss: 0.2354 - accuracy: 0.9206 - val_loss: 0.2676 - val_accuracy: 0.9085\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 84s 145ms/step - loss: 0.2364 - accuracy: 0.9215 - val_loss: 0.2667 - val_accuracy: 0.9054\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 79s 135ms/step - loss: 0.2315 - accuracy: 0.9216 - val_loss: 0.2672 - val_accuracy: 0.9087\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 78s 135ms/step - loss: 0.2294 - accuracy: 0.9224 - val_loss: 0.2679 - val_accuracy: 0.9095\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 77s 133ms/step - loss: 0.2279 - accuracy: 0.9214 - val_loss: 0.2663 - val_accuracy: 0.9098\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 79s 135ms/step - loss: 0.2251 - accuracy: 0.9235 - val_loss: 0.2676 - val_accuracy: 0.9064\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 79s 135ms/step - loss: 0.2244 - accuracy: 0.9244 - val_loss: 0.2665 - val_accuracy: 0.9066\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 77s 133ms/step - loss: 0.2227 - accuracy: 0.9234 - val_loss: 0.2702 - val_accuracy: 0.9054\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 77s 133ms/step - loss: 0.2204 - accuracy: 0.9248 - val_loss: 0.2698 - val_accuracy: 0.9041\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 77s 133ms/step - loss: 0.2185 - accuracy: 0.9276 - val_loss: 0.2772 - val_accuracy: 0.9045\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "#creating the model \n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "#dropout can be varied as per efficiency. I have kept it at a standard 0.6\n",
    "model.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
    "model.add(layers.Dense(3,activation='softmax'))\n",
    "#I have used a different optimizer here, since I have already tried the adam optimizer\n",
    "#Hyperparameters can be varied as per efficiency\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxJUosspF7Iv",
    "outputId": "af9bbbc2-4a2b-4346-ccc1-542169c14007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 - 4s - loss: 0.2772 - accuracy: 0.9045 - 4s/epoch - 22ms/step\n"
     ]
    }
   ],
   "source": [
    "#using evaluate function to get loss and validation accuracy obtained\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "DGsn7ZAYPNDr"
   },
   "outputs": [],
   "source": [
    "#converting the output predicted y, which is a vector of probabilities into a one-hot encoded output\n",
    "for i in range(len(y_pred)):\n",
    "  for j in range(3):\n",
    "    if(y_pred[i][j]<0.5):\n",
    "      y_pred[i][j]=0.0\n",
    "    else:\n",
    "      y_pred[i][j]=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI7a0_sGi5nP"
   },
   "source": [
    "Output from a Bi-Directional LSTM Model ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "uYgyA4LaQbd8",
    "outputId": "662f2654-adc4-45d5-d280-f2f0836848f8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.52      0.08      0.13       359\\n           1       0.92      0.97      0.94      4800\\n           2       0.87      0.87      0.87      1037\\n\\n   micro avg       0.91      0.90      0.90      6196\\n   macro avg       0.77      0.64      0.65      6196\\nweighted avg       0.89      0.90      0.88      6196\\n samples avg       0.91      0.90      0.90      6196\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classification_report(y_test, y_pred,zero_division=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
